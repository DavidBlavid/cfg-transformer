{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from nltk import CFG\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_grammar = [\n",
    "'S -> RULE',\n",
    "'RULE -> NONTERMINAL \" -> \" CONTENT',\n",
    "'CONTENT -> CONTENT \" | \" CONTENT | TERMINAL_PRE | TERMINAL_PRE \" \" NONTERMINAL | NONTERMINAL \" \" TERMINAL_PRE | NONTERMINAL \" \" NONTERMINAL | TERMINAL_PRE \" \" TERMINAL_PRE | TERMINAL_PRE \" \" NONTERMINAL \" \" TERMINAL_PRE',\n",
    "'TERMINAL_PRE -> \"#\" TERMINAL \"#\" ',    # \"#\" gets replaced with double quotes later\n",
    "# 'TERMINAL -> \"a\" | \"b\" | \"c\" | \"d\" | \"e\" | \"f\" | \"g\" | \"h\" | \"i\" | \"j\" | \"k\" | \"l\" | \"m\" | \"n\" | \"o\" | \"p\" | \"q\" | \"r\" | \"s\" | \"t\" | \"u\" | \"v\" | \"w\" | \"x\" | \"y\" | \"z\"',\n",
    "# 'NONTERMINAL -> \"A\" | \"B\" | \"C\" | \"D\" | \"E\" | \"F\" | \"G\" | \"H\" | \"I\" | \"J\" | \"K\" | \"L\" | \"M\" | \"N\" | \"O\" | \"P\" | \"Q\" | \"R\" | \"S\" | \"T\" | \"U\" | \"V\" | \"W\" | \"X\" | \"Y\" | \"Z\"',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a random valid sentence from the grammar\n",
    "def generate_random_sentence(grammar, join_char=' '):\n",
    "\n",
    "    # how often to try to generate a valid sentence\n",
    "    # if we fail, we raise a ValueError\n",
    "    max_tries = 5\n",
    "\n",
    "    # how often to try to expand a non-terminal\n",
    "    # after this many iterations, we give up and try again\n",
    "    max_iterations = 10000\n",
    "\n",
    "    # Ensure the grammar is an CFG object\n",
    "    if isinstance(grammar, str) or isinstance(grammar, list):\n",
    "        grammar = CFG.fromstring(grammar)\n",
    "    elif not isinstance(grammar, CFG):\n",
    "        raise ValueError(\"The grammar must be a string or an CFG object.\")\n",
    "\n",
    "    for current_try in range(max_tries):\n",
    "\n",
    "        current_iteration = 0\n",
    "        sentence = [grammar.start()]\n",
    "\n",
    "        # as long as there are non-terminals in the sentence\n",
    "        while any(nltk.grammar.is_nonterminal(symbol) for symbol in sentence):\n",
    "\n",
    "            # find the non-terminals\n",
    "            non_terminals = [i for i, symbol in enumerate(sentence) if nltk.grammar.is_nonterminal(symbol)]\n",
    "\n",
    "            if not non_terminals:\n",
    "                break  # we are done, there are only non-terminals in the sentence\n",
    "\n",
    "            # randomly choose a non-terminal to expand\n",
    "            nt_index = random.choice(non_terminals)\n",
    "            symbol = sentence[nt_index]\n",
    "\n",
    "            # randomly choose a production for the non-terminal\n",
    "            productions = grammar.productions(lhs=symbol)\n",
    "            \n",
    "            production = random.choice(productions)\n",
    "\n",
    "            # replace the non-terminal with the production\n",
    "            sentence = sentence[:nt_index] + list(production.rhs()) + sentence[nt_index+1:]\n",
    "\n",
    "            # avoid infinite loops\n",
    "            current_iteration += 1\n",
    "            if current_iteration > max_iterations:\n",
    "                break\n",
    "        \n",
    "        # if the sentence is valid, return it\n",
    "        if not any(nltk.grammar.is_nonterminal(symbol) for symbol in sentence):\n",
    "            return join_char.join(str(symbol) for symbol in sentence)\n",
    "\n",
    "    raise ValueError(\"The grammar is too complex to generate a valid sentence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function generates a random grammar\n",
    "def generate_random_grammar(terminals, nonterminals, n_rules=5):\n",
    "    assert n_rules >= len(nonterminals), \"There must be at least as many rules as nonterminals (n_rules >= len(nonterminals))\"\n",
    "\n",
    "    meta_rules = meta_grammar.copy()\n",
    "\n",
    "    # generate rules for terminals\n",
    "    for terminal in terminals:\n",
    "        rule = f'TERMINAL -> \"{terminal}\"'\n",
    "        meta_rules.append(rule)\n",
    "    \n",
    "    # generate rules for nonterminals\n",
    "    for nonterminal in nonterminals:\n",
    "        rule = f'NONTERMINAL -> \"{nonterminal}\"'\n",
    "        meta_rules.append(rule)\n",
    "\n",
    "    # generate rules\n",
    "    random_rules = []\n",
    "\n",
    "    # the first n_nonterminal rules start with the nonterminals\n",
    "    # so that every nonterminal has at least one rule\n",
    "    for nonterminal in nonterminals:\n",
    "        # generate one CONTENT and build the rule like this:\n",
    "        random_content = generate_random_sentence(meta_rules, join_char='')\n",
    "        # replace the first character of the content with the nonterminal\n",
    "        random_rule = nonterminal + random_content[1:]\n",
    "\n",
    "        # replace all '#' with '\"\"\n",
    "        random_rule = random_rule.replace('#', '\"')\n",
    "\n",
    "        random_rules.append(random_rule)\n",
    "\n",
    "    while len(random_rules) < n_rules:\n",
    "        random_rule = generate_random_sentence(meta_rules, join_char='')\n",
    "\n",
    "        if random_rule not in random_rules:\n",
    "            random_rule = random_rule.replace('#', '\"')\n",
    "            random_rules.append(random_rule)\n",
    "    \n",
    "    # sort the rules\n",
    "    random_rules.sort()\n",
    "\n",
    "    # add the starting rule to the beginning of the list\n",
    "    starting_rule = 'S -> ' + nonterminals[0]\n",
    "    random_rules.insert(0, starting_rule)\n",
    "    \n",
    "    # convert the rules to a string\n",
    "    random_rules = '\\n'.join(random_rules)\n",
    "\n",
    "    return random_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checks if a sentence is in the grammar\n",
    "def sentence_in_grammar(sentence: str, grammar) -> bool:\n",
    "    \n",
    "    # Ensure the grammar is an CFG object\n",
    "    if isinstance(grammar, str) or isinstance(grammar, list):\n",
    "        grammar = CFG.fromstring(grammar)\n",
    "    elif not isinstance(grammar, CFG):\n",
    "        raise ValueError(\"The grammar must be a string or an CFG object.\")\n",
    "    \n",
    "    # split the sentence into a char array\n",
    "    tokens = list(sentence)\n",
    "    \n",
    "    # Initialize the parser with the given grammar\n",
    "    parser = nltk.parse.EarleyChartParser(grammar)\n",
    "    \n",
    "    # Attempt to parse the tokenized sentence\n",
    "    try:\n",
    "        # Generate all possible parse trees for the sentence\n",
    "        for parse in parser.parse(tokens):\n",
    "            # If at least one parse tree is found, the sentence can be generated by the grammar\n",
    "            return True\n",
    "    except ValueError as e:\n",
    "        # Catch and handle the case where the sentence contains tokens not in the grammar\n",
    "        return False\n",
    "    \n",
    "    # If no parse trees are found, the sentence cannot be generated by the grammar\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a generator that generates all valid strings for a given grammar until a given length\n",
    "def generate_valid_strings(terminals, grammar, max_length=5):\n",
    "    \n",
    "    # Ensure the grammar is an CFG object\n",
    "    if isinstance(grammar, str) or isinstance(grammar, list):\n",
    "        grammar = CFG.fromstring(grammar)\n",
    "    elif not isinstance(grammar, CFG):\n",
    "        raise ValueError(\"The grammar must be a string or an CFG object.\")\n",
    "\n",
    "    # Iterate over all lengths of strings\n",
    "    for length in range(1, max_length + 1):\n",
    "        # Generate all combinations with repetition of the current length\n",
    "        for word in itertools.product(terminals, repeat=length):\n",
    "            # Join the characters to form a string\n",
    "            sentence = ''.join(word)\n",
    "\n",
    "            # Check if the sentence is in the grammar\n",
    "            in_grammar = sentence_in_grammar(sentence, grammar)\n",
    "\n",
    "            # If the sentence is in the grammar, yield it\n",
    "            if in_grammar:\n",
    "                yield sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a list of uppercase letters\n",
    "def generate_nonterminals(n: int):\n",
    "    return [chr(65+i) for i in range(n)]\n",
    "\n",
    "# generate a list lowercase letters\n",
    "def generate_terminals(n: int):\n",
    "    return [chr(97+i) for i in range(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns true, if the grammar will loop endlessly\n",
    "# implemented by checking whether the graph of the rules is transient\n",
    "def build_transition_graph(grammar):\n",
    "    \"\"\"Build a transition graph from the grammar for Markov chain analysis.\"\"\"\n",
    "    graph = defaultdict(list)\n",
    "    for production in grammar.productions():\n",
    "        lhs = str(production.lhs())\n",
    "        rhs_symbols = [str(rhs) for rhs in production.rhs() if nltk.grammar.is_nonterminal(rhs)]\n",
    "        graph[lhs].extend(rhs_symbols)\n",
    "    return graph\n",
    "\n",
    "def find_reachable_states(graph, start='S'):\n",
    "    \"\"\"Find all states reachable from the start symbol using DFS.\"\"\"\n",
    "    reachable = set()\n",
    "    stack = [start]\n",
    "    while stack:\n",
    "        current = stack.pop()\n",
    "        if current not in reachable:\n",
    "            reachable.add(current)\n",
    "            stack.extend(graph[current])\n",
    "    return reachable\n",
    "\n",
    "def find_absorbing_states(grammar):\n",
    "    \"\"\"Identify terminal symbols (absorbing states) in the grammar.\"\"\"\n",
    "    return {str(production.lhs()) for production in grammar.productions() if all(not nltk.grammar.is_nonterminal(rhs) for rhs in production.rhs())}\n",
    "\n",
    "def is_grammar_transient(grammar_input):\n",
    "    \"\"\"Check if the grammar is transient using Markov chain analysis.\"\"\"\n",
    "    if isinstance(grammar_input, str) or isinstance(grammar_input, list):\n",
    "        grammar = CFG.fromstring('\\n'.join(grammar_input) if isinstance(grammar_input, list) else grammar_input)\n",
    "    elif not isinstance(grammar_input, CFG):\n",
    "        raise ValueError(\"The grammar must be a string or an CFG object.\")\n",
    "    \n",
    "    graph = build_transition_graph(grammar)\n",
    "    absorbing_states = find_absorbing_states(grammar)\n",
    "    reachable_states = find_reachable_states(graph)\n",
    "    \n",
    "    # Initialize all non-terminals as transient until proven otherwise\n",
    "    transient = {node: True for node in graph}\n",
    "    \n",
    "    # Markov chain analysis: Check reachability of absorbing states\n",
    "    for start in graph:\n",
    "        visited = set()\n",
    "        stack = [start]\n",
    "\n",
    "        # check if start is a reachable state\n",
    "        # if not, we will never generate it\n",
    "        # we can ignore it and give it the value True\n",
    "        if start not in reachable_states:\n",
    "            transient[start] = True\n",
    "            continue\n",
    "\n",
    "        while stack:\n",
    "            current = stack.pop()\n",
    "            if current in absorbing_states:\n",
    "                break  # Found path to absorbing state\n",
    "            if current not in visited:\n",
    "                visited.add(current)\n",
    "                stack.extend(graph[current])\n",
    "        else:\n",
    "            transient[start] = False  # No path to absorbing state found\n",
    "    \n",
    "    return all(transient.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph: defaultdict(<class 'list'>, {'S': ['A', 'D'], 'A': [], 'B': ['C'], 'C': ['B'], 'D': []})\n",
      "Reachable states: {'D', 'S', 'A'}\n",
      "Absorbing states: {'D', 'A'}\n",
      "Is transient: True\n"
     ]
    }
   ],
   "source": [
    "# True, despite B and C loop, S can reach terminals through A and D\n",
    "mixed_starting_nonterminals = [\n",
    "    'S -> A | D',\n",
    "    'A -> \"a\"',\n",
    "    'B -> C',\n",
    "    'C -> B',\n",
    "    'D -> \"d\"',\n",
    "]\n",
    "\n",
    "grammar = CFG.fromstring('\\n'.join(mixed_starting_nonterminals))\n",
    "graph = build_transition_graph(grammar)\n",
    "reachable_states = find_reachable_states(graph, start='S')\n",
    "absorbing_states = find_absorbing_states(grammar)\n",
    "\n",
    "print(\"Graph:\", graph)\n",
    "print(\"Reachable states:\", reachable_states)\n",
    "print(\"Absorbing states:\", absorbing_states)\n",
    "print(\"Is transient:\", is_grammar_transient(mixed_starting_nonterminals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Test cases for is_grammar_transient\n",
    "# True, simple path to terminal\n",
    "transient_grammar = [\n",
    "    'S -> A',\n",
    "    'A -> B',\n",
    "    'B -> \"c\"',\n",
    "]\n",
    "\n",
    "# False, loop includes A again\n",
    "nontransient_grammar = [\n",
    "    'S -> A',\n",
    "    'A -> B',\n",
    "    'B -> \"c\" A',\n",
    "]\n",
    "\n",
    "# True, A has an alternative path to \"b\"\n",
    "direct_loop_with_terminal = [\n",
    "    'S -> A',\n",
    "    'A -> A | \"b\"',\n",
    "]\n",
    "\n",
    "# False, indirect loop without terminal resolution\n",
    "indirect_loop_without_terminal = [\n",
    "    'S -> A',\n",
    "    'A -> B',\n",
    "    'B -> C',\n",
    "    'C -> A',\n",
    "]\n",
    "\n",
    "# True, despite B and C loop, S can reach terminals through A and D\n",
    "mixed_starting_nonterminals = [\n",
    "    'S -> A | D',\n",
    "    'A -> \"a\"',\n",
    "    'B -> C',\n",
    "    'C -> B',\n",
    "    'D -> \"d\"',\n",
    "]\n",
    "\n",
    "# True, loops exist but all have escape to terminals\n",
    "nested_loops_with_escape = [\n",
    "    'S -> A',\n",
    "    'A -> B | \"e\"',\n",
    "    'B -> C',\n",
    "    'C -> D | B',\n",
    "    'D -> \"f\"',\n",
    "]\n",
    "\n",
    "# True, deep nesting but eventually reaches terminal\n",
    "deeply_nested_structure = [\n",
    "    'S -> A',\n",
    "    'A -> B',\n",
    "    'B -> C',\n",
    "    'C -> D',\n",
    "    'D -> E',\n",
    "    'E -> \"g\"',\n",
    "]\n",
    "\n",
    "# True, complex with multiple paths, some loops, some lead to terminals\n",
    "complex_grammar_multiple_paths = [\n",
    "    'S -> A | X',\n",
    "    'A -> B | \"h\"',\n",
    "    'B -> C | \"i\"',\n",
    "    'C -> A | \"j\"',\n",
    "    'X -> Y',\n",
    "    'Y -> Z | X',\n",
    "    'Z -> \"k\"',\n",
    "]\n",
    "\n",
    "                                                            # Expected output\n",
    "print(is_grammar_transient(transient_grammar))              # True\n",
    "print(is_grammar_transient(nontransient_grammar))           # False\n",
    "print(is_grammar_transient(direct_loop_with_terminal))      # True\n",
    "print(is_grammar_transient(indirect_loop_without_terminal)) # False\n",
    "print(is_grammar_transient(mixed_starting_nonterminals))    # True\n",
    "print(is_grammar_transient(nested_loops_with_escape))       # True\n",
    "print(is_grammar_transient(deeply_nested_structure))        # True\n",
    "print(is_grammar_transient(complex_grammar_multiple_paths)) # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_table(terminals, grammar, max_length=3):\n",
    "    \n",
    "    # Ensure the grammar is an CFG object\n",
    "    if isinstance(grammar, str) or isinstance(grammar, list):\n",
    "        grammar = CFG.fromstring(grammar)\n",
    "    elif not isinstance(grammar, CFG):\n",
    "        raise ValueError(\"The grammar must be a string or an CFG object.\")\n",
    "\n",
    "    # Iterate over all lengths of strings\n",
    "    for current_length in range(1, max_length+1):\n",
    "        # Generate all combinations with repetition of the current length\n",
    "        for word in itertools.product(terminals, repeat=current_length):\n",
    "            # Join the characters to form a string\n",
    "            sentence = ''.join(word)\n",
    "\n",
    "            # Check if the sentence is in the grammar\n",
    "            in_grammar = sentence_in_grammar(sentence, grammar)\n",
    "\n",
    "            # Print the sentence and whether it's in the grammar\n",
    "            print(f\"{sentence}: {in_grammar}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S -> A\n",
      "A -> \"游릱\" \"游린\"\n",
      "A -> \"游릳\" C\n",
      "B -> \"游린\" \"游릲\"\n",
      "B -> C B\n",
      "C -> \"游린\" \"游릳\"\n",
      "C -> \"游릱\" C\n",
      "C -> A \"游릳\" | \"游릳\" \"游릲\"\n",
      "C -> A B\n",
      "is_transient: True\n",
      "游릳游린游릳\n",
      "游릱游린\n",
      "游릱游린\n",
      "游릱游린\n",
      "游릱游린\n",
      "游릳游릱游린游릳\n",
      "游릱游린\n",
      "游릱游린\n",
      "游릳游린游릳\n",
      "游릳游릳游릲\n"
     ]
    }
   ],
   "source": [
    "# these parameters control the grammar\n",
    "n_nonterminals = 3\n",
    "n_terminals = 5\n",
    "n_rules = 8\n",
    "\n",
    "# generate the nonterminals and terminals\n",
    "nonterminals = generate_nonterminals(n_nonterminals)\n",
    "terminals = \"游린游릱游릳游릴游릲游릵游릶拘拘럻릢엃릢왫리游릭游멇릮游릯丘丘뾮"\n",
    "terminals = list(terminals)\n",
    "\n",
    "terminals = terminals[:n_terminals]\n",
    "\n",
    "grammar = generate_random_grammar(terminals, nonterminals, n_rules=n_rules)\n",
    "is_transient = is_grammar_transient(grammar)\n",
    "\n",
    "print(grammar)\n",
    "print(f'is_transient: {is_transient}')\n",
    "\n",
    "if is_transient:\n",
    "    for i in range(10):\n",
    "        sentence = generate_random_sentence(grammar, join_char='')\n",
    "        print(sentence)\n",
    "else:\n",
    "    print(\"The grammar is not transient, so it will loop endlessly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "游린: True\n",
      "游릱: False\n",
      "游릳: False\n",
      "游릴: False\n",
      "游릲: True\n",
      "游린游린: False\n",
      "游린游릱: False\n",
      "游린游릳: False\n",
      "游린游릴: False\n",
      "游린游릲: False\n",
      "游릱游린: False\n",
      "游릱游릱: False\n",
      "游릱游릳: False\n",
      "游릱游릴: False\n",
      "游릱游릲: False\n",
      "游릳游린: False\n",
      "游릳游릱: False\n",
      "游릳游릳: False\n",
      "游릳游릴: False\n",
      "游릳游릲: False\n",
      "游릴游린: False\n",
      "游릴游릱: False\n",
      "游릴游릳: False\n",
      "游릴游릴: False\n",
      "游릴游릲: False\n",
      "游릲游린: False\n",
      "游릲游릱: False\n",
      "游릲游릳: False\n",
      "游릲游릴: False\n",
      "游릲游릲: False\n",
      "游린游린游린: False\n",
      "游린游린游릱: False\n",
      "游린游린游릳: False\n",
      "游린游린游릴: False\n",
      "游린游린游릲: False\n",
      "游린游릱游린: False\n",
      "游린游릱游릱: False\n",
      "游린游릱游릳: False\n",
      "游린游릱游릴: False\n",
      "游린游릱游릲: False\n",
      "游린游릳游린: False\n",
      "游린游릳游릱: False\n",
      "游린游릳游릳: False\n",
      "游린游릳游릴: False\n",
      "游린游릳游릲: False\n",
      "游린游릴游린: False\n",
      "游린游릴游릱: False\n",
      "游린游릴游릳: False\n",
      "游린游릴游릴: False\n",
      "游린游릴游릲: False\n",
      "游린游릲游린: False\n",
      "游린游릲游릱: False\n",
      "游린游릲游릳: False\n",
      "游린游릲游릴: False\n",
      "游린游릲游릲: False\n",
      "游릱游린游린: False\n",
      "游릱游린游릱: False\n",
      "游릱游린游릳: False\n",
      "游릱游린游릴: False\n",
      "游릱游린游릲: False\n",
      "游릱游릱游린: False\n",
      "游릱游릱游릱: False\n",
      "游릱游릱游릳: False\n",
      "游릱游릱游릴: False\n",
      "游릱游릱游릲: False\n",
      "游릱游릳游린: False\n",
      "游릱游릳游릱: False\n",
      "游릱游릳游릳: False\n",
      "游릱游릳游릴: False\n",
      "游릱游릳游릲: False\n",
      "游릱游릴游린: False\n",
      "游릱游릴游릱: False\n",
      "游릱游릴游릳: False\n",
      "游릱游릴游릴: False\n",
      "游릱游릴游릲: False\n",
      "游릱游릲游린: False\n",
      "游릱游릲游릱: False\n",
      "游릱游릲游릳: False\n",
      "游릱游릲游릴: False\n",
      "游릱游릲游릲: False\n",
      "游릳游린游린: False\n",
      "游릳游린游릱: False\n",
      "游릳游린游릳: False\n",
      "游릳游린游릴: False\n",
      "游릳游린游릲: False\n",
      "游릳游릱游린: False\n",
      "游릳游릱游릱: False\n",
      "游릳游릱游릳: False\n",
      "游릳游릱游릴: False\n",
      "游릳游릱游릲: False\n",
      "游릳游릳游린: False\n",
      "游릳游릳游릱: False\n",
      "游릳游릳游릳: False\n",
      "游릳游릳游릴: False\n",
      "游릳游릳游릲: False\n",
      "游릳游릴游린: False\n",
      "游릳游릴游릱: False\n",
      "游릳游릴游릳: False\n",
      "游릳游릴游릴: False\n",
      "游릳游릴游릲: False\n",
      "游릳游릲游린: False\n",
      "游릳游릲游릱: False\n",
      "游릳游릲游릳: False\n",
      "游릳游릲游릴: False\n",
      "游릳游릲游릲: False\n",
      "游릴游린游린: False\n",
      "游릴游린游릱: False\n",
      "游릴游린游릳: False\n",
      "游릴游린游릴: False\n",
      "游릴游린游릲: False\n",
      "游릴游릱游린: False\n",
      "游릴游릱游릱: False\n",
      "游릴游릱游릳: False\n",
      "游릴游릱游릴: False\n",
      "游릴游릱游릲: False\n",
      "游릴游릳游린: False\n",
      "游릴游릳游릱: False\n",
      "游릴游릳游릳: False\n",
      "游릴游릳游릴: False\n",
      "游릴游릳游릲: False\n",
      "游릴游릴游린: True\n",
      "游릴游릴游릱: False\n",
      "游릴游릴游릳: False\n",
      "游릴游릴游릴: False\n",
      "游릴游릴游릲: False\n",
      "游릴游릲游린: False\n",
      "游릴游릲游릱: False\n",
      "游릴游릲游릳: False\n",
      "游릴游릲游릴: False\n",
      "游릴游릲游릲: False\n",
      "游릲游린游린: False\n",
      "游릲游린游릱: False\n",
      "游릲游린游릳: False\n",
      "游릲游린游릴: False\n",
      "游릲游린游릲: True\n",
      "游릲游릱游린: False\n",
      "游릲游릱游릱: False\n",
      "游릲游릱游릳: False\n",
      "游릲游릱游릴: False\n",
      "游릲游릱游릲: False\n",
      "游릲游릳游린: False\n",
      "游릲游릳游릱: False\n",
      "游릲游릳游릳: False\n",
      "游릲游릳游릴: False\n",
      "游릲游릳游릲: False\n",
      "游릲游릴游린: False\n",
      "游릲游릴游릱: False\n",
      "游릲游릴游릳: False\n",
      "游릲游릴游릴: False\n",
      "游릲游릴游릲: False\n",
      "游릲游릲游린: False\n",
      "游릲游릲游릱: False\n",
      "游릲游릲游릳: False\n",
      "游릲游릲游릴: False\n",
      "游릲游릲游릲: True\n"
     ]
    }
   ],
   "source": [
    "# try all strings and print whether they are in the grammar\n",
    "word_table(terminals, grammar=grammar, max_length=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "游린\n",
      "游릲\n",
      "游릴游릴游린\n",
      "游릲游린游릲\n",
      "游릲游릲游릲\n",
      "游린游릴游릴游린\n",
      "游릴游릴游릴游린\n",
      "游릴游릴游릴游릲\n",
      "游릲游릴游릴游린\n",
      "游린游린游릴游릴游린\n",
      "游린游릴游릴游릴游린\n",
      "游린游릴游릴游릴游릲\n",
      "游린游릲游릴游릴游린\n",
      "游릲游린游릴游릴游린\n",
      "游릲游릴游릴游린游릲\n",
      "游릲游릴游릴游릴游린\n",
      "游릲游릴游릴游릴游릲\n",
      "游릲游릲游린游릲游릲\n",
      "游릲游릲游릴游릴游린\n",
      "游릲游릲游릲游릲游릲\n"
     ]
    }
   ],
   "source": [
    "# print all valid strings of length 5\n",
    "for sentence in generate_valid_strings(terminals, grammar, max_length=5):\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write 1.000.000 sentences to a file\n",
    "if False:\n",
    "    with open('sentences.txt', 'w') as f:\n",
    "        for _ in tqdm(range(1_000_000)):\n",
    "            f.write(generate_random_sentence(grammar, '') + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
