{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from nltk import CFG\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_grammar = [\n",
    "'S -> RULE',\n",
    "'RULE -> NONTERMINAL \" -> \" CONTENT',\n",
    "'CONTENT -> CONTENT \" | \" CONTENT | TERMINAL_PRE | TERMINAL_PRE \" \" NONTERMINAL | NONTERMINAL \" \" TERMINAL_PRE | NONTERMINAL \" \" NONTERMINAL | TERMINAL_PRE \" \" TERMINAL_PRE | TERMINAL_PRE \" \" NONTERMINAL \" \" TERMINAL_PRE',\n",
    "'TERMINAL_PRE -> \"#\" TERMINAL \"#\" ',    # \"#\" gets replaced with double quotes later\n",
    "# 'TERMINAL -> \"a\" | \"b\" | \"c\" | \"d\" | \"e\" | \"f\" | \"g\" | \"h\" | \"i\" | \"j\" | \"k\" | \"l\" | \"m\" | \"n\" | \"o\" | \"p\" | \"q\" | \"r\" | \"s\" | \"t\" | \"u\" | \"v\" | \"w\" | \"x\" | \"y\" | \"z\"',\n",
    "# 'NONTERMINAL -> \"A\" | \"B\" | \"C\" | \"D\" | \"E\" | \"F\" | \"G\" | \"H\" | \"I\" | \"J\" | \"K\" | \"L\" | \"M\" | \"N\" | \"O\" | \"P\" | \"Q\" | \"R\" | \"S\" | \"T\" | \"U\" | \"V\" | \"W\" | \"X\" | \"Y\" | \"Z\"',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a random valid sentence from the grammar\n",
    "def generate_random_sentence(grammar, join_char=' '):\n",
    "\n",
    "    # how often to try to generate a valid sentence\n",
    "    # if we fail, we raise a ValueError\n",
    "    max_tries = 5\n",
    "\n",
    "    # how often to try to expand a non-terminal\n",
    "    # after this many iterations, we give up and try again\n",
    "    max_iterations = 10000\n",
    "\n",
    "    # Ensure the grammar is an CFG object\n",
    "    if isinstance(grammar, str) or isinstance(grammar, list):\n",
    "        grammar = CFG.fromstring(grammar)\n",
    "    elif not isinstance(grammar, CFG):\n",
    "        raise ValueError(\"The grammar must be a string or an CFG object.\")\n",
    "\n",
    "    for current_try in range(max_tries):\n",
    "\n",
    "        current_iteration = 0\n",
    "        sentence = [grammar.start()]\n",
    "\n",
    "        # as long as there are non-terminals in the sentence\n",
    "        while any(nltk.grammar.is_nonterminal(symbol) for symbol in sentence):\n",
    "\n",
    "            # find the non-terminals\n",
    "            non_terminals = [i for i, symbol in enumerate(sentence) if nltk.grammar.is_nonterminal(symbol)]\n",
    "\n",
    "            if not non_terminals:\n",
    "                break  # we are done, there are only non-terminals in the sentence\n",
    "\n",
    "            # randomly choose a non-terminal to expand\n",
    "            nt_index = random.choice(non_terminals)\n",
    "            symbol = sentence[nt_index]\n",
    "\n",
    "            # randomly choose a production for the non-terminal\n",
    "            productions = grammar.productions(lhs=symbol)\n",
    "            \n",
    "            production = random.choice(productions)\n",
    "\n",
    "            # replace the non-terminal with the production\n",
    "            sentence = sentence[:nt_index] + list(production.rhs()) + sentence[nt_index+1:]\n",
    "\n",
    "            # avoid infinite loops\n",
    "            current_iteration += 1\n",
    "            if current_iteration > max_iterations:\n",
    "                break\n",
    "        \n",
    "        # if the sentence is valid, return it\n",
    "        if not any(nltk.grammar.is_nonterminal(symbol) for symbol in sentence):\n",
    "            return join_char.join(str(symbol) for symbol in sentence)\n",
    "\n",
    "    raise ValueError(\"The grammar is too complex to generate a valid sentence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function generates a random grammar\n",
    "def generate_random_grammar(terminals, nonterminals, n_rules=5):\n",
    "    assert n_rules >= len(nonterminals), \"There must be at least as many rules as nonterminals (n_rules >= len(nonterminals))\"\n",
    "\n",
    "    meta_rules = meta_grammar.copy()\n",
    "\n",
    "    # generate rules for terminals\n",
    "    for terminal in terminals:\n",
    "        rule = f'TERMINAL -> \"{terminal}\"'\n",
    "        meta_rules.append(rule)\n",
    "    \n",
    "    # generate rules for nonterminals\n",
    "    for nonterminal in nonterminals:\n",
    "        rule = f'NONTERMINAL -> \"{nonterminal}\"'\n",
    "        meta_rules.append(rule)\n",
    "\n",
    "    # generate rules\n",
    "    random_rules = []\n",
    "\n",
    "    # the first n_nonterminal rules start with the nonterminals\n",
    "    # so that every nonterminal has at least one rule\n",
    "    for nonterminal in nonterminals:\n",
    "        # generate one CONTENT and build the rule like this:\n",
    "        random_content = generate_random_sentence(meta_rules, join_char='')\n",
    "        # replace the first character of the content with the nonterminal\n",
    "        random_rule = nonterminal + random_content[1:]\n",
    "\n",
    "        # replace all '#' with '\"\"\n",
    "        random_rule = random_rule.replace('#', '\"')\n",
    "\n",
    "        random_rules.append(random_rule)\n",
    "\n",
    "    while len(random_rules) < n_rules:\n",
    "        random_rule = generate_random_sentence(meta_rules, join_char='')\n",
    "\n",
    "        if random_rule not in random_rules:\n",
    "            random_rule = random_rule.replace('#', '\"')\n",
    "            random_rules.append(random_rule)\n",
    "    \n",
    "    # sort the rules\n",
    "    random_rules.sort()\n",
    "\n",
    "    # add the starting rule to the beginning of the list\n",
    "    starting_rule = 'S -> ' + nonterminals[0]\n",
    "    random_rules.insert(0, starting_rule)\n",
    "    \n",
    "    # convert the rules to a string\n",
    "    random_rules = '\\n'.join(random_rules)\n",
    "\n",
    "    return random_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checks if a sentence is in the grammar\n",
    "def sentence_in_grammar(sentence: str, grammar) -> bool:\n",
    "    \n",
    "    # Ensure the grammar is an CFG object\n",
    "    if isinstance(grammar, str) or isinstance(grammar, list):\n",
    "        grammar = CFG.fromstring(grammar)\n",
    "    elif not isinstance(grammar, CFG):\n",
    "        raise ValueError(\"The grammar must be a string or an CFG object.\")\n",
    "    \n",
    "    # split the sentence into a char array\n",
    "    tokens = list(sentence)\n",
    "    \n",
    "    # Initialize the parser with the given grammar\n",
    "    parser = nltk.parse.EarleyChartParser(grammar)\n",
    "    \n",
    "    # Attempt to parse the tokenized sentence\n",
    "    try:\n",
    "        # Generate all possible parse trees for the sentence\n",
    "        for parse in parser.parse(tokens):\n",
    "            # If at least one parse tree is found, the sentence can be generated by the grammar\n",
    "            return True\n",
    "    except ValueError as e:\n",
    "        # Catch and handle the case where the sentence contains tokens not in the grammar\n",
    "        return False\n",
    "    \n",
    "    # If no parse trees are found, the sentence cannot be generated by the grammar\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a generator that generates all valid strings for a given grammar until a given length\n",
    "def generate_valid_strings(terminals, grammar, max_length=5):\n",
    "    \n",
    "    # Ensure the grammar is an CFG object\n",
    "    if isinstance(grammar, str) or isinstance(grammar, list):\n",
    "        grammar = CFG.fromstring(grammar)\n",
    "    elif not isinstance(grammar, CFG):\n",
    "        raise ValueError(\"The grammar must be a string or an CFG object.\")\n",
    "\n",
    "    # Iterate over all lengths of strings\n",
    "    for length in range(1, max_length + 1):\n",
    "        # Generate all combinations with repetition of the current length\n",
    "        for word in itertools.product(terminals, repeat=length):\n",
    "            # Join the characters to form a string\n",
    "            sentence = ''.join(word)\n",
    "\n",
    "            # Check if the sentence is in the grammar\n",
    "            in_grammar = sentence_in_grammar(sentence, grammar)\n",
    "\n",
    "            # If the sentence is in the grammar, yield it\n",
    "            if in_grammar:\n",
    "                yield sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a list of uppercase letters\n",
    "def generate_nonterminals(n: int):\n",
    "    return [chr(65+i) for i in range(n)]\n",
    "\n",
    "# generate a list lowercase letters\n",
    "def generate_terminals(n: int):\n",
    "    return [chr(97+i) for i in range(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns true, if the grammar will loop endlessly\n",
    "# implemented by checking whether the graph of the rules is transient\n",
    "def build_transition_graph(grammar):\n",
    "    \"\"\"Build a transition graph from the grammar for Markov chain analysis.\"\"\"\n",
    "    graph = defaultdict(list)\n",
    "    for production in grammar.productions():\n",
    "        lhs = str(production.lhs())\n",
    "        rhs_symbols = [str(rhs) for rhs in production.rhs() if nltk.grammar.is_nonterminal(rhs)]\n",
    "        graph[lhs].extend(rhs_symbols)\n",
    "    return graph\n",
    "\n",
    "def find_reachable_states(graph, start='S'):\n",
    "    \"\"\"Find all states reachable from the start symbol using DFS.\"\"\"\n",
    "    reachable = set()\n",
    "    stack = [start]\n",
    "    while stack:\n",
    "        current = stack.pop()\n",
    "        if current not in reachable:\n",
    "            reachable.add(current)\n",
    "            stack.extend(graph[current])\n",
    "    return reachable\n",
    "\n",
    "def find_absorbing_states(grammar):\n",
    "    \"\"\"Identify terminal symbols (absorbing states) in the grammar.\"\"\"\n",
    "    return {str(production.lhs()) for production in grammar.productions() if all(not nltk.grammar.is_nonterminal(rhs) for rhs in production.rhs())}\n",
    "\n",
    "def is_grammar_transient(grammar_input):\n",
    "    \"\"\"Check if the grammar is transient using Markov chain analysis.\"\"\"\n",
    "    if isinstance(grammar_input, str) or isinstance(grammar_input, list):\n",
    "        grammar = CFG.fromstring('\\n'.join(grammar_input) if isinstance(grammar_input, list) else grammar_input)\n",
    "    elif not isinstance(grammar_input, CFG):\n",
    "        raise ValueError(\"The grammar must be a string or an CFG object.\")\n",
    "    \n",
    "    graph = build_transition_graph(grammar)\n",
    "    absorbing_states = find_absorbing_states(grammar)\n",
    "    reachable_states = find_reachable_states(graph)\n",
    "    \n",
    "    # Initialize all non-terminals as transient until proven otherwise\n",
    "    transient = {node: True for node in graph}\n",
    "    \n",
    "    # Markov chain analysis: Check reachability of absorbing states\n",
    "    for start in graph:\n",
    "        visited = set()\n",
    "        stack = [start]\n",
    "\n",
    "        # check if start is a reachable state\n",
    "        # if not, we will never generate it\n",
    "        # we can ignore it and give it the value True\n",
    "        if start not in reachable_states:\n",
    "            transient[start] = True\n",
    "            continue\n",
    "\n",
    "        while stack:\n",
    "            current = stack.pop()\n",
    "            if current in absorbing_states:\n",
    "                break  # Found path to absorbing state\n",
    "            if current not in visited:\n",
    "                visited.add(current)\n",
    "                stack.extend(graph[current])\n",
    "        else:\n",
    "            transient[start] = False  # No path to absorbing state found\n",
    "    \n",
    "    return all(transient.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph: defaultdict(<class 'list'>, {'S': ['A', 'D'], 'A': [], 'B': ['C'], 'C': ['B'], 'D': []})\n",
      "Reachable states: {'D', 'S', 'A'}\n",
      "Absorbing states: {'D', 'A'}\n",
      "Is transient: True\n"
     ]
    }
   ],
   "source": [
    "# True, despite B and C loop, S can reach terminals through A and D\n",
    "mixed_starting_nonterminals = [\n",
    "    'S -> A | D',\n",
    "    'A -> \"a\"',\n",
    "    'B -> C',\n",
    "    'C -> B',\n",
    "    'D -> \"d\"',\n",
    "]\n",
    "\n",
    "grammar = CFG.fromstring('\\n'.join(mixed_starting_nonterminals))\n",
    "graph = build_transition_graph(grammar)\n",
    "reachable_states = find_reachable_states(graph, start='S')\n",
    "absorbing_states = find_absorbing_states(grammar)\n",
    "\n",
    "print(\"Graph:\", graph)\n",
    "print(\"Reachable states:\", reachable_states)\n",
    "print(\"Absorbing states:\", absorbing_states)\n",
    "print(\"Is transient:\", is_grammar_transient(mixed_starting_nonterminals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Test cases for is_grammar_transient\n",
    "# True, simple path to terminal\n",
    "transient_grammar = [\n",
    "    'S -> A',\n",
    "    'A -> B',\n",
    "    'B -> \"c\"',\n",
    "]\n",
    "\n",
    "# False, loop includes A again\n",
    "nontransient_grammar = [\n",
    "    'S -> A',\n",
    "    'A -> B',\n",
    "    'B -> \"c\" A',\n",
    "]\n",
    "\n",
    "# True, A has an alternative path to \"b\"\n",
    "direct_loop_with_terminal = [\n",
    "    'S -> A',\n",
    "    'A -> A | \"b\"',\n",
    "]\n",
    "\n",
    "# False, indirect loop without terminal resolution\n",
    "indirect_loop_without_terminal = [\n",
    "    'S -> A',\n",
    "    'A -> B',\n",
    "    'B -> C',\n",
    "    'C -> A',\n",
    "]\n",
    "\n",
    "# True, despite B and C loop, S can reach terminals through A and D\n",
    "mixed_starting_nonterminals = [\n",
    "    'S -> A | D',\n",
    "    'A -> \"a\"',\n",
    "    'B -> C',\n",
    "    'C -> B',\n",
    "    'D -> \"d\"',\n",
    "]\n",
    "\n",
    "# True, loops exist but all have escape to terminals\n",
    "nested_loops_with_escape = [\n",
    "    'S -> A',\n",
    "    'A -> B | \"e\"',\n",
    "    'B -> C',\n",
    "    'C -> D | B',\n",
    "    'D -> \"f\"',\n",
    "]\n",
    "\n",
    "# True, deep nesting but eventually reaches terminal\n",
    "deeply_nested_structure = [\n",
    "    'S -> A',\n",
    "    'A -> B',\n",
    "    'B -> C',\n",
    "    'C -> D',\n",
    "    'D -> E',\n",
    "    'E -> \"g\"',\n",
    "]\n",
    "\n",
    "# True, complex with multiple paths, some loops, some lead to terminals\n",
    "complex_grammar_multiple_paths = [\n",
    "    'S -> A | X',\n",
    "    'A -> B | \"h\"',\n",
    "    'B -> C | \"i\"',\n",
    "    'C -> A | \"j\"',\n",
    "    'X -> Y',\n",
    "    'Y -> Z | X',\n",
    "    'Z -> \"k\"',\n",
    "]\n",
    "\n",
    "                                                            # Expected output\n",
    "print(is_grammar_transient(transient_grammar))              # True\n",
    "print(is_grammar_transient(nontransient_grammar))           # False\n",
    "print(is_grammar_transient(direct_loop_with_terminal))      # True\n",
    "print(is_grammar_transient(indirect_loop_without_terminal)) # False\n",
    "print(is_grammar_transient(mixed_starting_nonterminals))    # True\n",
    "print(is_grammar_transient(nested_loops_with_escape))       # True\n",
    "print(is_grammar_transient(deeply_nested_structure))        # True\n",
    "print(is_grammar_transient(complex_grammar_multiple_paths)) # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_table(terminals, grammar, max_length=3):\n",
    "    \n",
    "    # Ensure the grammar is an CFG object\n",
    "    if isinstance(grammar, str) or isinstance(grammar, list):\n",
    "        grammar = CFG.fromstring(grammar)\n",
    "    elif not isinstance(grammar, CFG):\n",
    "        raise ValueError(\"The grammar must be a string or an CFG object.\")\n",
    "\n",
    "    # Iterate over all lengths of strings\n",
    "    for current_length in range(1, max_length+1):\n",
    "        # Generate all combinations with repetition of the current length\n",
    "        for word in itertools.product(terminals, repeat=current_length):\n",
    "            # Join the characters to form a string\n",
    "            sentence = ''.join(word)\n",
    "\n",
    "            # Check if the sentence is in the grammar\n",
    "            in_grammar = sentence_in_grammar(sentence, grammar)\n",
    "\n",
    "            # Print the sentence and whether it's in the grammar\n",
    "            print(f\"{sentence}: {in_grammar}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S -> A\n",
      "A -> \"🟦\" \"🟥\"\n",
      "A -> \"🟨\" C\n",
      "B -> \"🟥\" \"🟧\"\n",
      "B -> C B\n",
      "C -> \"🟥\" \"🟨\"\n",
      "C -> \"🟦\" C\n",
      "C -> A \"🟨\" | \"🟨\" \"🟧\"\n",
      "C -> A B\n",
      "is_transient: True\n",
      "🟨🟥🟨\n",
      "🟦🟥\n",
      "🟦🟥\n",
      "🟦🟥\n",
      "🟦🟥\n",
      "🟨🟦🟥🟨\n",
      "🟦🟥\n",
      "🟦🟥\n",
      "🟨🟥🟨\n",
      "🟨🟨🟧\n"
     ]
    }
   ],
   "source": [
    "# these parameters control the grammar\n",
    "n_nonterminals = 3\n",
    "n_terminals = 5\n",
    "n_rules = 8\n",
    "\n",
    "# generate the nonterminals and terminals\n",
    "nonterminals = generate_nonterminals(n_nonterminals)\n",
    "terminals = \"🟥🟦🟨🟩🟧🟪🟫⬛⬜🔴🔵🟡🟢🟠🟣🟤⚫⚪\"\n",
    "terminals = list(terminals)\n",
    "\n",
    "terminals = terminals[:n_terminals]\n",
    "\n",
    "grammar = generate_random_grammar(terminals, nonterminals, n_rules=n_rules)\n",
    "is_transient = is_grammar_transient(grammar)\n",
    "\n",
    "print(grammar)\n",
    "print(f'is_transient: {is_transient}')\n",
    "\n",
    "if is_transient:\n",
    "    for i in range(10):\n",
    "        sentence = generate_random_sentence(grammar, join_char='')\n",
    "        print(sentence)\n",
    "else:\n",
    "    print(\"The grammar is not transient, so it will loop endlessly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🟥: True\n",
      "🟦: False\n",
      "🟨: False\n",
      "🟩: False\n",
      "🟧: True\n",
      "🟥🟥: False\n",
      "🟥🟦: False\n",
      "🟥🟨: False\n",
      "🟥🟩: False\n",
      "🟥🟧: False\n",
      "🟦🟥: False\n",
      "🟦🟦: False\n",
      "🟦🟨: False\n",
      "🟦🟩: False\n",
      "🟦🟧: False\n",
      "🟨🟥: False\n",
      "🟨🟦: False\n",
      "🟨🟨: False\n",
      "🟨🟩: False\n",
      "🟨🟧: False\n",
      "🟩🟥: False\n",
      "🟩🟦: False\n",
      "🟩🟨: False\n",
      "🟩🟩: False\n",
      "🟩🟧: False\n",
      "🟧🟥: False\n",
      "🟧🟦: False\n",
      "🟧🟨: False\n",
      "🟧🟩: False\n",
      "🟧🟧: False\n",
      "🟥🟥🟥: False\n",
      "🟥🟥🟦: False\n",
      "🟥🟥🟨: False\n",
      "🟥🟥🟩: False\n",
      "🟥🟥🟧: False\n",
      "🟥🟦🟥: False\n",
      "🟥🟦🟦: False\n",
      "🟥🟦🟨: False\n",
      "🟥🟦🟩: False\n",
      "🟥🟦🟧: False\n",
      "🟥🟨🟥: False\n",
      "🟥🟨🟦: False\n",
      "🟥🟨🟨: False\n",
      "🟥🟨🟩: False\n",
      "🟥🟨🟧: False\n",
      "🟥🟩🟥: False\n",
      "🟥🟩🟦: False\n",
      "🟥🟩🟨: False\n",
      "🟥🟩🟩: False\n",
      "🟥🟩🟧: False\n",
      "🟥🟧🟥: False\n",
      "🟥🟧🟦: False\n",
      "🟥🟧🟨: False\n",
      "🟥🟧🟩: False\n",
      "🟥🟧🟧: False\n",
      "🟦🟥🟥: False\n",
      "🟦🟥🟦: False\n",
      "🟦🟥🟨: False\n",
      "🟦🟥🟩: False\n",
      "🟦🟥🟧: False\n",
      "🟦🟦🟥: False\n",
      "🟦🟦🟦: False\n",
      "🟦🟦🟨: False\n",
      "🟦🟦🟩: False\n",
      "🟦🟦🟧: False\n",
      "🟦🟨🟥: False\n",
      "🟦🟨🟦: False\n",
      "🟦🟨🟨: False\n",
      "🟦🟨🟩: False\n",
      "🟦🟨🟧: False\n",
      "🟦🟩🟥: False\n",
      "🟦🟩🟦: False\n",
      "🟦🟩🟨: False\n",
      "🟦🟩🟩: False\n",
      "🟦🟩🟧: False\n",
      "🟦🟧🟥: False\n",
      "🟦🟧🟦: False\n",
      "🟦🟧🟨: False\n",
      "🟦🟧🟩: False\n",
      "🟦🟧🟧: False\n",
      "🟨🟥🟥: False\n",
      "🟨🟥🟦: False\n",
      "🟨🟥🟨: False\n",
      "🟨🟥🟩: False\n",
      "🟨🟥🟧: False\n",
      "🟨🟦🟥: False\n",
      "🟨🟦🟦: False\n",
      "🟨🟦🟨: False\n",
      "🟨🟦🟩: False\n",
      "🟨🟦🟧: False\n",
      "🟨🟨🟥: False\n",
      "🟨🟨🟦: False\n",
      "🟨🟨🟨: False\n",
      "🟨🟨🟩: False\n",
      "🟨🟨🟧: False\n",
      "🟨🟩🟥: False\n",
      "🟨🟩🟦: False\n",
      "🟨🟩🟨: False\n",
      "🟨🟩🟩: False\n",
      "🟨🟩🟧: False\n",
      "🟨🟧🟥: False\n",
      "🟨🟧🟦: False\n",
      "🟨🟧🟨: False\n",
      "🟨🟧🟩: False\n",
      "🟨🟧🟧: False\n",
      "🟩🟥🟥: False\n",
      "🟩🟥🟦: False\n",
      "🟩🟥🟨: False\n",
      "🟩🟥🟩: False\n",
      "🟩🟥🟧: False\n",
      "🟩🟦🟥: False\n",
      "🟩🟦🟦: False\n",
      "🟩🟦🟨: False\n",
      "🟩🟦🟩: False\n",
      "🟩🟦🟧: False\n",
      "🟩🟨🟥: False\n",
      "🟩🟨🟦: False\n",
      "🟩🟨🟨: False\n",
      "🟩🟨🟩: False\n",
      "🟩🟨🟧: False\n",
      "🟩🟩🟥: True\n",
      "🟩🟩🟦: False\n",
      "🟩🟩🟨: False\n",
      "🟩🟩🟩: False\n",
      "🟩🟩🟧: False\n",
      "🟩🟧🟥: False\n",
      "🟩🟧🟦: False\n",
      "🟩🟧🟨: False\n",
      "🟩🟧🟩: False\n",
      "🟩🟧🟧: False\n",
      "🟧🟥🟥: False\n",
      "🟧🟥🟦: False\n",
      "🟧🟥🟨: False\n",
      "🟧🟥🟩: False\n",
      "🟧🟥🟧: True\n",
      "🟧🟦🟥: False\n",
      "🟧🟦🟦: False\n",
      "🟧🟦🟨: False\n",
      "🟧🟦🟩: False\n",
      "🟧🟦🟧: False\n",
      "🟧🟨🟥: False\n",
      "🟧🟨🟦: False\n",
      "🟧🟨🟨: False\n",
      "🟧🟨🟩: False\n",
      "🟧🟨🟧: False\n",
      "🟧🟩🟥: False\n",
      "🟧🟩🟦: False\n",
      "🟧🟩🟨: False\n",
      "🟧🟩🟩: False\n",
      "🟧🟩🟧: False\n",
      "🟧🟧🟥: False\n",
      "🟧🟧🟦: False\n",
      "🟧🟧🟨: False\n",
      "🟧🟧🟩: False\n",
      "🟧🟧🟧: True\n"
     ]
    }
   ],
   "source": [
    "# try all strings and print whether they are in the grammar\n",
    "word_table(terminals, grammar=grammar, max_length=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🟥\n",
      "🟧\n",
      "🟩🟩🟥\n",
      "🟧🟥🟧\n",
      "🟧🟧🟧\n",
      "🟥🟩🟩🟥\n",
      "🟩🟩🟩🟥\n",
      "🟩🟩🟩🟧\n",
      "🟧🟩🟩🟥\n",
      "🟥🟥🟩🟩🟥\n",
      "🟥🟩🟩🟩🟥\n",
      "🟥🟩🟩🟩🟧\n",
      "🟥🟧🟩🟩🟥\n",
      "🟧🟥🟩🟩🟥\n",
      "🟧🟩🟩🟥🟧\n",
      "🟧🟩🟩🟩🟥\n",
      "🟧🟩🟩🟩🟧\n",
      "🟧🟧🟥🟧🟧\n",
      "🟧🟧🟩🟩🟥\n",
      "🟧🟧🟧🟧🟧\n"
     ]
    }
   ],
   "source": [
    "# print all valid strings of length 5\n",
    "for sentence in generate_valid_strings(terminals, grammar, max_length=5):\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write 1.000.000 sentences to a file\n",
    "if False:\n",
    "    with open('sentences.txt', 'w') as f:\n",
    "        for _ in tqdm(range(1_000_000)):\n",
    "            f.write(generate_random_sentence(grammar, '') + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
